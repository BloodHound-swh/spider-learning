# 爬虫学习

## 1.对爬取的内容进行替换处理



```python
class YangguangPipeline(object):
    def process_item(self, item, spider):
        item["content"] = self.process_content(item["content"])
        return item
    
    def process_content(self, content):
        content = [re.sub(r"\xa0|\s", "", i) for i in content]
        content = [i for i in content if len(i) > 0]  # 去除列表中的空字符串
        return content
```



## 2.如何把未完成的数据传到详情页中去

使用<font size=6>**meta**</font>参数

```python
yield scrapy.Request(
	item["href"],
    callback=self.parse_detail,
    meta = {"detail": item}
)

def parse_detail(self, response):
    item = response.meta["detail"]
    ...
```



## 3.xpath基础

**下面列出了最有用的路径表达式：**

| 表达式   | 描述                                                       |
| -------- | ---------------------------------------------------------- |
| nodename | 选取此节点的所有子节点。                                   |
| /        | 从根节点选取。                                             |
| //       | 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。 |
| .        | 选取当前节点。                                             |
| ..       | 选取当前节点的父节点。                                     |
| @        | 选取属性。                                                 |



在下面的表格中，列出了一些路径表达式以及表达式的结果：

| 路径表达式      | 结果                                                         |
| --------------- | :----------------------------------------------------------- |
| bookstore       | 选取 bookstore 元素的所有子节点。                            |
| /bookstore      | 选取根元素 bookstore。注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！ |
| bookstore/book  | 选取属于 bookstore 的子元素的所有 book 元素。                |
| //book          | 选取所有 book 子元素，而不管它们在文档中的位置。             |
| bookstore//book | 选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置。 |
| //@lang         | 选取名为 lang 的所有属性。                                   |



在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果：

| 路径表达式                         | 结果                                                         |
| ---------------------------------- | ------------------------------------------------------------ |
| /bookstore/book[1]                 | 选取属于 bookstore 子元素的第一个 book 元素。                |
| /bookstore/book[last()]            | 选取属于 bookstore 子元素的最后一个 book 元素。              |
| /bookstore/book[last()-1]          | 选取属于 bookstore 子元素的倒数第二个 book 元素。            |
| /bookstore/book[position()<3]      | 选取最前面的两个属于 bookstore 元素的子元素的 book 元素。    |
| //title[@lang]                     | 选取所有拥有名为 lang 的属性的 title 元素。                  |
| //title[@lang='eng']               | 选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。   |
| /bookstore/book[price>35.00]       | 选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。 |
| /bookstore/book[price>35.00]/title | 选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。 |





## 4.使用CrawlSpider

生成CrawlSpider的命令：

```cql
scrapy genspider -t crawl csdn "csdn.cn"
```



```python
rules = (
	Rule(LinkExtractor(allow=r'http://bolg.csdn.net/\d+\.htm'), callback=my,parse,follow=True)
)
```

callback将通过正则表达式提取的url送到指定的函数中处理（此时一般用不到follow）

follow则表示是否继续按照这个规则提取url，一般用于下一页中（此时一般用不到callback）



## 5.cookies

首先在浏览器里找到cookies的字符串，然后用复写start_request(self)函数

```python
def start_requests(self):
    cookies = "**=**; ---=--; +++=++;";
	cookies = {i.split("=")[0]:i.split("=")[1] for i in cookies.split("; ")}
    yield scrapy.Request(
    	self.start_urls[0],
        callback=self.parse,
        cookies=cookies
    )
```





## 6.下载中间件（可以用来处理常见的反爬虫问题）

首先自己建立好一个`USER_ANGENTS_LIST`在setting.py中

之后再middlewares.py中

```python
import random

class RandomUserAgentMiddleware:
    def process_request(self, request, spider):
        ua = random.choice(spider.settings.get("USER_AGNETS_LIST"))
        request.headers["User-Agent"] = ua
        
        
class CheckUserAgent:
    def process_response(self, request, response, spider):
        print(request.headers["User-Agent"])
        return response
```

然后再在setting.py中将DOWNLOADER_MIDDLEWARES的注释打开，并把自己创建的类放进去



## 7.login

使用`scrapy.FormRequest.from_response`

```python
def parse(self, response):
    yield scrapy.FormRequest.from_response(
    	response, # 自动的从response中寻找form表单
        formdata={"login":"bloodhoundswh", "password":"Swh123456789"},
        callback = self.after_login
    )
```

或者